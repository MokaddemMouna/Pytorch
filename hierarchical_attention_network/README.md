- This is an implementation of Hierarchical Attention Network by Yan and al. in this [paper](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf). I chose this model because I already experimented with Bahdanau's [attention](https://arxiv.org/abs/1409.0473) for a Neural [Machine translation task](https://github.com/MokaddemMouna/Pytorch/tree/master/Bahdanau_attention) (from German to English) and I found that it gives very good performance. But the former model is based on the sequence to sequence architecture so I wanted to explore if there is any attention mechanism that could be applied for document classification. I found the HAN model. So I read the paper and I found that the model does what I was looking for, the hierarchical architecture together with the two level-attention mechanism makes it focus on words that are relevent and then sentences enabling it to attend differentially to more and less important content when constructing the document representation. The two most important characteristics of this model that makes it suit our task is i-the hierarchical architecture that reflects the document structure, ii-the attention mechanism applied at the word-level and sentence-level.

- The model works as the following: it takes as input the list of tokenized sentences, then it calculates the attention weights by applying softmax of the dot product of a context vector (this vector is initialized randomly and will be trained along the task) and a linear projections of the bidirectional hidden vectors. Then each sentence representation is calculated as the weighted sum of the bidirectional hidden vectors. The same attention mechnisms then will be applied with sentence vectors just calculated before to get a document vector (again there is a sentence context vector that it is initialized randomly and trained along the task). At the end, a linear layer is applied on the document vector and then a softmax to get the probability of classes (topics). Categorical crossentropy is used as the loss function and adam algorith as the optimizer. Dropout is used at the both levels word-level and sentence-level. 

- The code I present here is inspired from this github [repository](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Text-Classification). I found that he has a nice architecture when implementing HAN. I took his code, I refactored it, improved/corrected it in some parts and further documented it. Then I added a han_classify.py file which takes a file of samples as input and returns the corresponding predicted classes by the trained model. I also added the app.py for deployment.
